import tensorflow as tf
import pickle

# Créer et configurer le tokenizer
tokenizer = tf.keras.preprocessing.text.Tokenizer(
    num_words=5000,  # Correspond à votre input_dim
    oov_token="<OOV>"  # Out of vocabulary token
)

# Adapter le tokenizer à vos données d'entraînement
# Remplacez texts_for_training par vos données d'entraînement nettoyées
tokenizer.fit_on_texts(texts_for_training)

# Sauvegarder le tokenizer
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)